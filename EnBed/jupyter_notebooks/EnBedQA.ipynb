{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import logging\n",
    "import ast\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_sqlite_db_to_dataframe(db_path:str) -> pd.DataFrame:\n",
    "    logging.basicConfig(filename='logs.log',\n",
    "                        level=logging.INFO,\n",
    "                        format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        logging.info(f'Established SQLite connection with: {db_path}')\n",
    "        query = \"SELECT * FROM computer_organization_and_design_table\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        df['embeddings'] = df['embeddings'].apply(ast.literal_eval)\n",
    "        df['embeddings'] = np.stack(df['embeddings'].to_numpy()).tolist()\n",
    "        logging.info('Database loaded and embeddings converted')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading database: {e}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        conn.close()\n",
    "        logging.info('Database connection closed')\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T11:22:00.230289Z",
     "start_time": "2024-02-10T11:21:59.252564Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarities(df_embeddings, single_embedding):\n",
    "    # Ensure df_embeddings is a 2D numpy array\n",
    "    embeddings_array = np.array(df_embeddings.tolist())\n",
    "\n",
    "    single_embedding_reshaped = np.reshape(single_embedding, (1, -1))\n",
    "    print(single_embedding_reshaped.shape)\n",
    "\n",
    "    # Calculate cosine similarity and directly convert to pandas Series.\n",
    "    similarities = cosine_similarity(embeddings_array, single_embedding_reshaped).flatten()\n",
    "    return pd.Series(similarities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T11:24:34.904081Z",
     "start_time": "2024-02-10T11:24:34.882588Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def embed_text(text):\n",
    "    response = client.embeddings.create(\n",
    "        input = text,\n",
    "        model = \"text-embedding-3-large\"\n",
    "    )\n",
    "    print(\"Embedding created.\")\n",
    "    return response.data[0].embedding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T11:24:35.793575Z",
     "start_time": "2024-02-10T11:24:35.663884Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def search_by_embeddings(dataframe, user_question, n=5):\n",
    "    embeded_user_q = embed_text(user_question)\n",
    "    print(\"embeded_user_q created.\")\n",
    "    \n",
    "    similarity_df = calculate_cosine_similarities(dataframe[\"embeddings\"], embeded_user_q)\n",
    "    print(\"similarity_df created.\")\n",
    "    \n",
    "    dataframe['similarities'] = similarity_df\n",
    "    print(\"dataframe['similarities'] created.\")\n",
    "    print(dataframe.head(5))\n",
    "    \n",
    "    res = dataframe.sort_values('similarities', ascending=False).head(n)\n",
    "    print(res.head(5))\n",
    "    return res\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T11:24:36.340200Z",
     "start_time": "2024-02-10T11:24:36.190509Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding created.\n",
      "embeded_user_q created.\n",
      "(1, 3072)\n",
      "similarity_df created.\n",
      "dataframe['similarities'] created.\n",
      "   id                                  guid  \\\n",
      "0   1  a9084a5b-9900-48bb-bc17-69dc6a749d90   \n",
      "1   2  214be393-5d82-440e-93e0-bda330cd38b7   \n",
      "2   3  309cedf6-56ae-4f27-bad1-fa8ca679d333   \n",
      "3   4  b1b7bee4-58e5-47f8-a499-38e9badfc362   \n",
      "4   5  13ee0133-cd22-4ca8-a554-a1b5f1686290   \n",
      "\n",
      "                                    author  \\\n",
      "0  David A. Patterson and John L. Hennessy   \n",
      "1  David A. Patterson and John L. Hennessy   \n",
      "2  David A. Patterson and John L. Hennessy   \n",
      "3  David A. Patterson and John L. Hennessy   \n",
      "4  David A. Patterson and John L. Hennessy   \n",
      "\n",
      "                                               title  target_page page_range  \\\n",
      "0  Computer Organization and Design: The Hardware...            1      0,1,2   \n",
      "1  Computer Organization and Design: The Hardware...            2      1,2,3   \n",
      "2  Computer Organization and Design: The Hardware...            3      2,3,4   \n",
      "3  Computer Organization and Design: The Hardware...            4      3,4,5   \n",
      "4  Computer Organization and Design: The Hardware...            5      4,5,6   \n",
      "\n",
      "                                             summary  \\\n",
      "0  The textbook \"Computer Organization and Design...   \n",
      "1  The textbook \"Computer Organization and Design...   \n",
      "2  The first page of the textbook \"Computer Organ...   \n",
      "3  David A. Patterson and John L. Hennessy, renow...   \n",
      "4  David A. Patterson, a distinguished computer a...   \n",
      "\n",
      "                                                text  \\\n",
      "0  In Praise of Computer Organization and Design:...   \n",
      "1  In Praise of Computer Organization and Design:...   \n",
      "2  This page intentionally left blank\\n\\n<<<Page ...   \n",
      "3  Computer Organization and Design\\nTHE HARDWARE...   \n",
      "4  David A. Patterson has been teaching computer ...   \n",
      "\n",
      "                                          embeddings  similarities  \n",
      "0  [0.0062266988679766655, -0.004798176698386669,...      0.228511  \n",
      "1  [0.017452556639909744, -0.004213566891849041, ...      0.236265  \n",
      "2  [0.02242455445230007, -0.013324006460607052, -...      0.277709  \n",
      "3  [0.0198462326079607, -0.005737158004194498, -0...      0.256435  \n",
      "4  [0.016869667917490005, -0.010021831840276718, ...      0.258223  \n",
      "      id                                  guid  \\\n",
      "298  304  77023abd-d368-4652-85d0-a3d7fd954d56   \n",
      "297  303  66fa499c-0ed2-4cce-b419-12d9283591a4   \n",
      "301  307  772aa48e-da6a-47be-b349-9e4b94e6cc2b   \n",
      "305  311  e98c11f6-5123-4ae8-acb2-7cba9c4f3f2e   \n",
      "299  305  c4477ab4-cb12-48d1-b825-a591c703899d   \n",
      "\n",
      "                                      author  \\\n",
      "298  David A. Patterson and John L. Hennessy   \n",
      "297  David A. Patterson and John L. Hennessy   \n",
      "301  David A. Patterson and John L. Hennessy   \n",
      "305  David A. Patterson and John L. Hennessy   \n",
      "299  David A. Patterson and John L. Hennessy   \n",
      "\n",
      "                                                 title  target_page  \\\n",
      "298  Computer Organization and Design: The Hardware...          300   \n",
      "297  Computer Organization and Design: The Hardware...          299   \n",
      "301  Computer Organization and Design: The Hardware...          303   \n",
      "305  Computer Organization and Design: The Hardware...          307   \n",
      "299  Computer Organization and Design: The Hardware...          301   \n",
      "\n",
      "      page_range                                            summary  \\\n",
      "298  299,300,301  The text discusses the concept of pipelining i...   \n",
      "297  298,299,300  In Chapter 4 of the computer organization and ...   \n",
      "301  302,303,304  In Chapter 4 of the computer organization and ...   \n",
      "305  306,307,308  In Chapter 4 of the computer organization and ...   \n",
      "299  300,301,302  The MIPS instruction set is designed for pipel...   \n",
      "\n",
      "                                                  text  \\\n",
      "298  <<<Page 277>>>\\n\\n4.5 An Overview of Pipelinin...   \n",
      "297  <<<Page 276>>>\\nChapter 4 The Processor\\nMoreo...   \n",
      "301  <<<Page 280>>>\\nChapter 4 The Processor\\nThe d...   \n",
      "305  <<<Page 284>>>\\nChapter 4 The Processor\\nA mor...   \n",
      "299  <<<Page 278>>>\\nChapter 4 The Processor\\nAs we...   \n",
      "\n",
      "                                            embeddings  similarities  \n",
      "298  [-0.025684287771582603, -0.026315633207559586,...      0.715859  \n",
      "297  [-0.01857064478099346, -0.021258311346173286, ...      0.689278  \n",
      "301  [-0.018343299627304077, -0.027950964868068695,...      0.647384  \n",
      "305  [-0.015254388563334942, -0.020520690828561783,...      0.632410  \n",
      "299  [-0.022771215066313744, -0.01981910690665245, ...      0.609524  \n",
      "      id                                  guid  \\\n",
      "298  304  77023abd-d368-4652-85d0-a3d7fd954d56   \n",
      "297  303  66fa499c-0ed2-4cce-b419-12d9283591a4   \n",
      "301  307  772aa48e-da6a-47be-b349-9e4b94e6cc2b   \n",
      "305  311  e98c11f6-5123-4ae8-acb2-7cba9c4f3f2e   \n",
      "299  305  c4477ab4-cb12-48d1-b825-a591c703899d   \n",
      "\n",
      "                                      author  \\\n",
      "298  David A. Patterson and John L. Hennessy   \n",
      "297  David A. Patterson and John L. Hennessy   \n",
      "301  David A. Patterson and John L. Hennessy   \n",
      "305  David A. Patterson and John L. Hennessy   \n",
      "299  David A. Patterson and John L. Hennessy   \n",
      "\n",
      "                                                 title  target_page  \\\n",
      "298  Computer Organization and Design: The Hardware...          300   \n",
      "297  Computer Organization and Design: The Hardware...          299   \n",
      "301  Computer Organization and Design: The Hardware...          303   \n",
      "305  Computer Organization and Design: The Hardware...          307   \n",
      "299  Computer Organization and Design: The Hardware...          301   \n",
      "\n",
      "      page_range                                            summary  \\\n",
      "298  299,300,301  The text discusses the concept of pipelining i...   \n",
      "297  298,299,300  In Chapter 4 of the computer organization and ...   \n",
      "301  302,303,304  In Chapter 4 of the computer organization and ...   \n",
      "305  306,307,308  In Chapter 4 of the computer organization and ...   \n",
      "299  300,301,302  The MIPS instruction set is designed for pipel...   \n",
      "\n",
      "                                                  text  \\\n",
      "298  <<<Page 277>>>\\n\\n4.5 An Overview of Pipelinin...   \n",
      "297  <<<Page 276>>>\\nChapter 4 The Processor\\nMoreo...   \n",
      "301  <<<Page 280>>>\\nChapter 4 The Processor\\nThe d...   \n",
      "305  <<<Page 284>>>\\nChapter 4 The Processor\\nA mor...   \n",
      "299  <<<Page 278>>>\\nChapter 4 The Processor\\nAs we...   \n",
      "\n",
      "                                            embeddings  similarities  \n",
      "298  [-0.025684287771582603, -0.026315633207559586,...      0.715859  \n",
      "297  [-0.01857064478099346, -0.021258311346173286, ...      0.689278  \n",
      "301  [-0.018343299627304077, -0.027950964868068695,...      0.647384  \n",
      "305  [-0.015254388563334942, -0.020520690828561783,...      0.632410  \n",
      "299  [-0.022771215066313744, -0.01981910690665245, ...      0.609524  \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = load_sqlite_db_to_dataframe('/Users/jdo/EnBed/EnBed/db/EnBed.sqlite')  # Provide a clear path as an argument\n",
    "search_query = \"How do I design instruction sets for pipelining in MIPS? In this same context, what is the first type of 'hazard'?\"\n",
    "results = search_by_embeddings(df, search_query)\n",
    "print(results)\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T11:24:50.708176Z",
     "start_time": "2024-02-10T11:24:37.307660Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298    <<<Page 277>>>\\n\\n4.5 An Overview of Pipelinin...\n",
      "297    <<<Page 276>>>\\nChapter 4 The Processor\\nMoreo...\n",
      "301    <<<Page 280>>>\\nChapter 4 The Processor\\nThe d...\n",
      "305    <<<Page 284>>>\\nChapter 4 The Processor\\nA mor...\n",
      "299    <<<Page 278>>>\\nChapter 4 The Processor\\nAs we...\n",
      "Name: text, dtype: object\n",
      "Text 1: <<<Page 277>>>\n",
      "\n",
      "4.5 An Overview of Pipelining \n",
      "Pipelining improves performance by increasing instruction throughput, as opposed to decreasing the execution time of an individual instruction, but instruction throughput is the important metric because real programs execute billions of instructions.\n",
      "\n",
      "Designing Instruction Sets for Pipelining\n",
      "Even with this simple explanation of pipelining, we can get insight into the design of the MIPS instruction set, which was designed for pipelined execution. First, all MIPS instructions are the same length. This restriction makes it much easier to fetch instructions in the first pipeline stage and to decode them in the second stage. In an instruction set like the x86, where instructions vary from 1 byte to 15 bytes, pipelining is considerably more challenging. Recent implementations of the x86 architecture actually translate x86 instructions into simple operations that look like MIPS instructions and then pipeline the simple operations rather than the native x86 instructions! (See Section 4.10.)\n",
      "\n",
      "Second, MIPS has only a few instruction formats, with the source register fields being located in the same place in each instruction. This symmetry means that the second stage can begin reading the register file at the same time that the hardware is determining what type of instruction was fetched. If MIPS instruction formats were not symmetric, we would need to split stage 2, resulting in six pipeline stages. We will shortly see the downside of longer pipelines.\n",
      "\n",
      "Third, memory operands only appear in loads or stores in MIPS. This restriction means we can use the execute stage to calculate the memory address and then access memory in the following stage. If we could operate on the operands in memory, as in the x86, stages 3 and 4 would expand to an address stage, memory stage, and then execute stage.\n",
      "\n",
      "Fourth, as discussed in Chapter 2, operands must be aligned in memory. Hence, we need not worry about a single data transfer instruction requiring two data memory accesses; the requested data can be transferred between processor and memory in a single pipeline stage.\n",
      "\n",
      "Pipeline Hazards\n",
      "There are situations in pipelining when the next instruction cannot execute in the following clock cycle. These events are called hazards, and there are three different types.\n",
      "\n",
      "Hazards\n",
      "The first hazard is called a structural hazard. It means that the hardware cannot support the combination of instructions that we want to execute in the same clock cycle. A structural hazard in the laundry room would occur if we used a washer-dryer combination instead of a separate washer and dryer, or if our roommate was busy doing something else and wouldn’t put clothes away. Our carefully scheduled pipeline plans would then be foiled.\n",
      "\n",
      "`structural hazard` When a planned instruction cannot execute in the proper clock cycle because the hardware does not support the combination of instructions that are set to execute.\n",
      "\n",
      "<<<Page 278>>>\n",
      "\n",
      "Chapter 4 The Processor\n",
      "As we said above, the MIPS instruction set was designed to be pipelined, making it fairly easy for designers to avoid structural hazards when designing a pipeline. Suppose, however, that we had a single memory instead of two memories. If the pipeline in Figure 4.27 had a fourth instruction, we would see that in the same clock cycle the first instruction is accessing data from memory while the fourth instruction is fetching an instruction from that same memory. Without two memories, our pipeline could have a structural hazard.\n",
      "\n",
      "Data Hazards\n",
      "Data hazards occur when the pipeline must be stalled because one step must wait for another to complete. Suppose you found a sock at the folding station for which no match existed. One possible strategy is to run down to your room and search through your clothes bureau to see if you can find the match. Obviously, while you are doing the search, loads must wait that have completed drying and are ready to fold as well as those that have finished washing and are ready to dry.\n",
      "\n",
      "In a computer pipeline, data hazards arise from the dependence of one instruction on an earlier one that is still in the pipeline (a relationship that does not really exist when doing laundry). For example, suppose we have an add instruction followed immediately by a subtract instruction that uses the sum (`$s0`):\n",
      "```\n",
      "add $s0, $t0, $t1\n",
      "sub $t2, $s0, $t3\n",
      "```\n",
      "Without intervention, a data hazard could severely stall the pipeline. The add instruction doesn’t write its result until the fifth stage, meaning that we would have to waste three clock cycles in the pipeline. Although we could try to rely on compilers to remove all such hazards, the results would not be satisfactory. These dependences happen just too often and the delay is just too long to expect the compiler to rescue us from this dilemma.\n",
      "\n",
      "The primary solution is based on the observation that we don’t need to wait for the instruction to complete before trying to resolve the data hazard. For the code sequence above, as soon as the ALU creates the sum for the add, we can supply it as an input for the subtract. Adding extra hardware to retrieve the missing item early from the internal resources is called forwarding or bypassing.\n",
      "\n",
      "Forwarding with Two Instructions\n",
      "For the two instructions above, show what pipeline stages would be connected by forwarding. Use the drawing in Figure 4.28 to represent the datapath during the five stages of the pipeline. Align a copy of the datapath for each instruction, similar to the laundry pipeline in Figure 4.25.\n",
      "\n",
      "`data hazard` Also called a pipeline data hazard. When a planned instruction cannot execute in the proper clock cycle because data that is needed to execute the instruction is not yet available.\n",
      "\n",
      "`forwarding` Also called bypassing. A method of resolving a data hazard by retrieving the missing data element from internal buffers rather than waiting for it to arrive from programmer-visible registers or memory.\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "Text 2: <<<Page 276>>>\n",
      "Chapter 4 The Processor\n",
      "Moreover, even our claim of fourfold improvement for our example is not reflected in the total execution time for the three instructions: it’s 1400 ps versus 2400 ps. Of course, this is because the number of instructions is not large. What would happen if we increased the number of instructions? We could extend the previous figures to 1,000,003 instructions. We would add 1,000,000 instructions in the pipelined example; each instruction adds 200 ps to the total execution time. The total execution time would be 1,000,000 × 200 ps + 1400 ps, or 200,001,400 ps. In the nonpipelined example, we would add 1,000,000 instructions, each taking 800 ps, so total execution time would be 1,000,000 × 800 ps + 2400 ps, or 800,002,400 ps. Under these conditions, the ratio of total execution times for real programs on nonpipelined to pipelined processors is close to the ratio of times between instructions: `800002400/200001400 ps = 4.00`.\n",
      "\n",
      "Program execution order (in instructions):\n",
      "```\n",
      "lw  $1, 100($0)\n",
      "lw  $2, 200($0)\n",
      "lw  $3, 300($0)\n",
      "```\n",
      "Time: `1000, 1200, 1400, 200, 400, 600, 800, 1000, 1200, 1400, 200, 400, 600, 800, 1600, 1800`\n",
      "Instruction fetch, Data access, Reg, Instruction fetch, Data access, Reg, Instruction fetch, 800 ps, 800 ps, 800 ps\n",
      "\n",
      "Program execution order (in instructions):\n",
      "```\n",
      "lw  $1, 100($0)\n",
      "lw  $2, 200($0)\n",
      "lw  $3, 300($0)\n",
      "```\n",
      "Time: Instruction fetch, Data access, Reg, Instruction fetch, Instruction fetch, Data access, Reg, Data access, Reg, 200 ps, 200 ps, 200 ps, 200 ps, 200 ps, 200 ps, 200 ps, ALU, Reg, ALU, Reg, ALU, ALU, ALU, Reg, Reg, Reg\n",
      "\n",
      "FIGURE 4.27 Single-cycle, nonpipelined execution in top versus pipelined execution in bottom. Both use the same hardware components, whose time is listed in Figure 4.26. In this case, we see a fourfold speed-up on average time between instructions, from 800 ps down to 200 ps. Compare this figure to Figure 4.25. For the laundry, we assumed all stages were equal. If the dryer were slowest, then the dryer stage would set the stage time. The pipeline stage times of a computer are also limited by the slowest resource, either the ALU operation or the memory access. We assume the write to the register file occurs in the first half of the clock cycle and the read from the register file occurs in the second half. We use this assumption throughout this chapter.\n",
      "\n",
      "<<<Page 277>>>\n",
      "4.5 An Overview of Pipelining\n",
      "Pipelining improves performance by increasing instruction throughput, as opposed to decreasing the execution time of an individual instruction, but instruction throughput is the important metric because real programs execute billions of instructions.\n",
      "\n",
      "Designing Instruction Sets for Pipelining\n",
      "Even with this simple explanation of pipelining, we can get insight into the design of the MIPS instruction set, which was designed for pipelined execution. First, all MIPS instructions are the same length. This restriction makes it much easier to fetch instructions in the first pipeline stage and to decode them in the second stage. In an instruction set like the x86, where instructions vary from 1 byte to 15 bytes, pipelining is considerably more challenging. Recent implementations of the x86 architecture actually translate x86 instructions into simple operations that look like MIPS instructions and then pipeline the simple operations rather than the native x86 instructions! (See Section 4.10.) Second, MIPS has only a few instruction formats, with the source register fields being located in the same place in each instruction. This symmetry means that the second stage can begin reading the register file at the same time that the hardware is determining what type of instruction was fetched. If MIPS instruction formats were not symmetric, we would need to split stage 2, resulting in six pipeline stages. We will shortly see the downside of longer pipelines. Third, memory operands only appear in loads or stores in MIPS. This restriction means we can use the execute stage to calculate the memory address and then access memory in the following stage. If we could operate on the operands in memory, as in the x86, stages 3 and 4 would expand to an address stage, memory stage, and then execute stage. Fourth, as discussed in Chapter 2, operands must be aligned in memory. Hence, we need not worry about a single data transfer instruction requiring two data memory accesses; the requested data can be transferred between processor and memory in a single pipeline stage.\n",
      "\n",
      "Pipeline Hazards\n",
      "There are situations in pipelining when the next instruction cannot execute in the following clock cycle. These events are called hazards, and there are three different types.\n",
      "\n",
      "Hazards\n",
      "The first hazard is called a structural hazard. It means that the hardware cannot support the combination of instructions that we want to execute in the same clock cycle. A structural hazard in the laundry room would occur if we used a washer-dryer combination instead of a separate washer and dryer, or if our roommate was busy doing something else and wouldn’t put clothes away. Our carefully scheduled pipeline plans would then be foiled. Structural hazard: When a planned instruction cannot execute in the proper clock cycle because the hardware does not support the combination of instructions that are set to execute.\n",
      "\n",
      "Text 3: <<<Page 280>>>\n",
      "Chapter 4 The Processor\n",
      "The desired data would be available only after the fourth stage of the first instruction in the dependence, which is too late for the input of the third stage of sub. Hence, even with forwarding, we would have to stall one stage for a load-use data hazard, as Figure 4.30 shows. This figure shows an important pipeline concept, officially called a pipeline stall, but often given the nickname bubble. We shall see stalls elsewhere in the pipeline. Section 4.7 shows how we can handle hard cases like these, using either hardware detection and stalls or software that reorders code to try to avoid load-use pipeline stalls, as this example illustrates.\n",
      "\n",
      "Reordering Code to Avoid Pipeline Stalls\n",
      "Consider the following code segment in C:\n",
      "```c\n",
      "a = b + e;\n",
      "c = b + f;\n",
      "```\n",
      "Here is the generated MIPS code for this segment, assuming all variables are in memory and are addressable as offsets from `$t0`:\n",
      "```\n",
      "lw    $t1, 0($t0)\n",
      "lw    $t2, 4($t0)\n",
      "add   $t3, $t1,$t2\n",
      "sw    $t3, 12($t0)\n",
      "lw    $t4, 8($t0)\n",
      "add   $t5, $t1,$t4\n",
      "sw    $t5, 16($t0)\n",
      "```\n",
      "Load-use data hazard: A specific form of data hazard in which the data being loaded by a load instruction has not yet become available when it is needed by another instruction.\n",
      "Pipeline stall: Also called bubble. A stall initiated in order to resolve a hazard.\n",
      "\n",
      "EXAMPLE\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "Time\n",
      "```\n",
      "lw $s0, 20($t1)\n",
      "sub $t2, $s0, $t3 \n",
      "IF\n",
      "MEM\n",
      "ID\n",
      "WB\n",
      "EX\n",
      "IF\n",
      "MEM\n",
      "ID\n",
      "WB\n",
      "EX\n",
      "Program\n",
      "execution\n",
      "order\n",
      "(in instructions)\n",
      "bubble\n",
      "bubble\n",
      "bubble\n",
      "bubble\n",
      "bubble\n",
      "```\n",
      "FIGURE 4.30 We need a stall even with forwarding when an R-format instruction following a load tries to use the data. Without the stall, the path from memory access stage output to execution stage input would be going backward in time, which is impossible. This figure is actually a simplification, since we cannot know until after the subtract instruction is fetched and decoded whether or not a stall will be necessary. Section 4.7 shows the details of what really happens in the case of a hazard.\n",
      "\n",
      "<<<Page 281>>>\n",
      "4.5 An Overview of Pipelining\n",
      "Find the hazards in the preceding code segment and reorder the instructions to avoid any pipeline stalls.\n",
      "Both add instructions have a hazard because of their respective dependence on the immediately preceding lw instruction. Notice that bypassing eliminates several other potential hazards, including the dependence of the first add on the first lw and any hazards for store instructions. Moving up the third lw instruction to become the third instruction eliminates both hazards:\n",
      "```\n",
      "lw   $t1, 0($t0)\n",
      "lw   $t2, 4($t0)\n",
      "lw   $t4, 8($t0)\n",
      "add  $t3, $t1,$t2\n",
      "sw   $t3, 12($t0)\n",
      "add  $t5, $t1,$t4\n",
      "sw   $t5, 16($t0)\n",
      "```\n",
      "On a pipelined processor with forwarding, the reordered sequence will complete in two fewer cycles than the original version.\n",
      "Forwarding yields another insight into the MIPS architecture, in addition to the four mentioned on page 277. Each MIPS instruction writes at most one result and does this in the last stage of the pipeline. Forwarding is harder if there are multiple results to forward per instruction or if there is a need to write a result early on in instruction execution.\n",
      "Elaboration: The name “forwarding” comes from the idea that the result is passed forward from an earlier instruction to a later instruction. “Bypassing” comes from passing the result around the register file to the desired unit.\n",
      "Control Hazards\n",
      "The third type of hazard is called a control hazard, arising from the need to make a decision based on the results of one instruction while others are executing.\n",
      "Suppose our laundry crew was given the happy task of cleaning the uniforms of a football team. Given how filthy the laundry is, we need to determine whether the detergent and water temperature setting we select is strong enough to get the uniforms clean but not so strong that the uniforms wear out sooner. In our laundry pipeline, we have to wait until after the second stage to examine the dry uniform to see if we need to change the washer setup or not. What to do?\n",
      "Here is the first of two solutions to control hazards in the laundry room and its computer equivalent.\n",
      "Stall: Just operate sequentially until the first batch is dry and then repeat until you have the right formula.\n",
      "This conservative option certainly works, but it is slow.\n",
      "ANSWER\n",
      "Control hazard: Also called branch hazard. When the proper instruction cannot execute in the proper pipeline clock cycle because the instruction that was fetched is not the one that is needed; that is, the flow of instruction addresses is not what the pipeline expected.\n",
      "\n",
      "Text 4: <<<Page 284>>>\n",
      "Chapter 4 The Processor\n",
      "A more sophisticated version of branch prediction would have some branches predicted as taken and some as untaken. In our analogy, the dark or home uniforms might take one formula while the light or road uniforms might take another. In the case of programming, at the bottom of loops are branches that jump back to the top of the loop. Since they are likely to be taken and they branch backward, we could always predict taken for branches that jump to an earlier address. Such rigid approaches to branch prediction rely on stereotypical behavior and don’t account for the individuality of a specific branch instruction. Dynamic hardware predictors, in stark contrast, make their guesses depending on the behavior of each branch and may change predictions for a branch over the life of a program. Following our analogy, in dynamic prediction a person would look at how dirty the uniform was and guess at the formula, adjusting the next prediction depending on the success of recent guesses. \n",
      "\n",
      "One popular approach to dynamic prediction of branches is keeping a history for each branch as taken or untaken, and then using the recent past behavior to predict the future. As we will see later, the amount and type of history kept have become extensive, with the result being that dynamic branch predictors can correctly predict branches with more than 90% accuracy (see Section 4.8). When the guess is wrong, the pipeline control must ensure that the instructions following the wrongly guessed branch have no effect and must restart the pipeline from the proper branch address. In our laundry analogy, we must stop taking new loads so that we can restart the load that we incorrectly predicted. \n",
      "\n",
      "As in the case of all other solutions to control hazards, longer pipelines exacerbate the problem, in this case by raising the cost of misprediction. Solutions to control hazards are described in more detail in Section 4.8. Elaboration: There is a third approach to the control hazard, called delayed decision. In our analogy, whenever you are going to make such a decision about laundry, just place a load of nonfootball clothes in the washer while waiting for football uniforms to dry. As long as you have enough dirty clothes that are not affected by the test, this solution works fine. \n",
      "\n",
      "Called the delayed branch in computers, and mentioned above, this is the solution actually used by the MIPS architecture. The delayed branch always executes the next sequential instruction, with the branch taking place after that one instruction delay. It is hidden from the MIPS assembly language programmer because the assembler can automatically arrange the instructions to get the branch behavior desired by the programmer. MIPS software will place an instruction immediately after the delayed branch instruction that is not affected by the branch, and a taken branch changes the address of the instruction that follows this safe instruction. In our example, the `add` instruction before the branch in Figure 4.31 does not affect the branch and can be moved after the branch to fully hide the branch delay. Since delayed branches are useful when the branches are short, no processor uses a delayed branch of more than one cycle. For longer branch delays, hardware-based branch prediction is usually used. \n",
      "\n",
      "`branch prediction`: A method of resolving a branch hazard that assumes a given outcome for the branch and proceeds from that assumption rather than waiting to ascertain the actual outcome.\n",
      "\n",
      "<<<Page 285>>>\n",
      "4.5 An Overview of Pipelining \n",
      "Pipeline Overview Summary\n",
      "Pipelining is a technique that exploits parallelism among the instructions in a sequential instruction stream. It has the substantial advantage that, unlike programming a multiprocessor, it is fundamentally invisible to the programmer. In the next few sections of this chapter, we cover the concept of pipelining using the MIPS instruction subset from the single-cycle implementation in Section 4.4 and show a simplified version of its pipeline. We then look at the problems that pipelining introduces and the performance attainable under typical situations. \n",
      "\n",
      "If you wish to focus more on the software and the performance implications of pipelining, you now have sufficient background to skip to Section 4.10. Section 4.10 introduces advanced pipelining concepts, such as superscalar and dynamic scheduling, and Section 4.11 examines the pipelines of recent microprocessors. Alternatively, if you are interested in understanding how pipelining is implemented and the challenges of dealing with hazards, you can proceed to examine the design of a pipelined datapath and the basic control, explained in Section 4.6. You can then use this understanding to explore the implementation of forwarding and stalls in Section 4.7. You can then read Section 4.8 to learn more about solutions to branch hazards, and then see how exceptions are handled in Section 4.9. \n",
      "\n",
      "For each code sequence below, state whether it must stall, can avoid stalls using only forwarding, or can execute without stalling or forwarding. \n",
      "\n",
      "Sequence 1: \n",
      "```\n",
      "lw $t0,0($t0)\n",
      "add $t1,$t0,$t0\n",
      "addi $t1,$t0,#1\n",
      "```\n",
      "Sequence 2: \n",
      "```\n",
      "add $t1,$t0,$t0\n",
      "addi $t2,$t0,#5\n",
      "addi $t4,$t1,#5\n",
      "```\n",
      "Sequence 3: \n",
      "```\n",
      "addi $t1,$t0,#1\n",
      "addi $t2,$t0,#2\n",
      "addi $t3,$t0,#2\n",
      "addi $t3,$t0,#4\n",
      "addi $t5,$t0,#5\n",
      "```\n",
      "Outside the memory system, the effective operation of the pipeline is usually the most important factor in determining the CPI of the processor and hence its performance. As we will see in Section 4.10, understanding the performance of a modern multiple-issue pipelined processor is complex and requires understanding more than just the issues that arise in a simple pipelined processor. Nonetheless, structural, data, and control hazards remain important in both simple pipelines and more sophisticated ones. \n",
      "\n",
      "For modern pipelines, structural hazards usually revolve around the floating-point unit, which may not be fully pipelined, while control hazards are usually more of a problem in integer programs, which tend to have higher branch frequencies as well as less predictable branches. Data hazards can be performance bottlenecks. Check Yourself Understanding Program Performance.\n",
      "\n",
      "Text 5: <<<Page 278>>>\n",
      "Chapter 4 The Processor\n",
      "As we said above, the MIPS instruction set was designed to be pipelined, making it fairly easy for designers to avoid structural hazards when designing a pipeline. Suppose, however, that we had a single memory instead of two memories. If the pipeline in Figure 4.27 had a fourth instruction, we would see that in the same clock cycle the first instruction is accessing data from memory while the fourth instruction is fetching an instruction from that same memory. Without two memories, our pipeline could have a structural hazard.\n",
      "\n",
      "Data Hazards\n",
      "Data hazards occur when the pipeline must be stalled because one step must wait for another to complete. Suppose you found a sock at the folding station for which no match existed. One possible strategy is to run down to your room and search through your clothes bureau to see if you can find the match. Obviously, while you are doing the search, loads must wait that have completed drying and are ready to fold as well as those that have finished washing and are ready to dry.\n",
      "\n",
      "In a computer pipeline, data hazards arise from the dependence of one instruction on an earlier one that is still in the pipeline (a relationship that does not really exist when doing laundry). For example, suppose we have an `add` instruction followed immediately by a `subtract` instruction that uses the sum (`$s0`):\n",
      "```\n",
      "add $s0, $t0, $t1\n",
      "sub $t2, $s0, $t3\n",
      "```\n",
      "Without intervention, a data hazard could severely stall the pipeline. The `add` instruction doesn’t write its result until the fifth stage, meaning that we would have to waste three clock cycles in the pipeline. Although we could try to rely on compilers to remove all such hazards, the results would not be satisfactory. These dependences happen just too often and the delay is just too long to expect the compiler to rescue us from this dilemma.\n",
      "\n",
      "The primary solution is based on the observation that we don’t need to wait for the instruction to complete before trying to resolve the data hazard. For the code sequence above, as soon as the ALU creates the sum for the `add`, we can supply it as an input for the `subtract`. Adding extra hardware to retrieve the missing item early from the internal resources is called forwarding or bypassing.\n",
      "\n",
      "Forwarding with Two Instructions\n",
      "For the two instructions above, show what pipeline stages would be connected by forwarding. Use the drawing in Figure 4.28 to represent the datapath during the five stages of the pipeline. Align a copy of the datapath for each instruction, similar to the laundry pipeline in Figure 4.25.\n",
      "\n",
      "Data hazard: Also called a pipeline data hazard. When a planned instruction cannot execute in the proper clock cycle because data that is needed to execute the instruction is not yet available.\n",
      "\n",
      "Forwarding: Also called bypassing. A method of resolving a data hazard by retrieving the missing data element from internal buffers rather than waiting for it to arrive from programmer-visible registers or memory.\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "<<<Page 279>>>\n",
      "Figure 4.29 shows the connection to forward the value in `$s0` after the execution stage of the `add` instruction as input to the execution stage of the `sub` instruction. In this graphical representation of events, forwarding paths are valid only if the destination stage is later in time than the source stage. For example, there cannot be a valid forwarding path from the output of the memory access stage in the first instruction to the input of the execution stage of the following, since that would mean going backward in time.\n",
      "\n",
      "Forwarding works very well and is described in detail in Section 4.7. It cannot prevent all pipeline stalls, however. For example, suppose the first instruction was a load of `$s0` instead of an `add`. As we can imagine from looking at Figure 4.29, the ANSWER Time `add $s0, $t0, $t1` IF MEM ID WB EX 200 400 600 800 1000 FIGURE 4.28 Graphical representation of the instruction pipeline, similar in spirit to the laundry pipeline in Figure 4.25. Here we use symbols representing the physical resources with the abbreviations for pipeline stages used throughout the chapter. The symbols for the five stages: IF for the instruction fetch stage, with the box representing instruction memory; ID for the instruction decode/register file read stage, with the drawing showing the register file being read; EX for the execution stage, with the drawing representing the ALU; MEM for the memory access stage, with the box representing data memory; and WB for the write-back stage, with the drawing showing the register file being written. The shading indicates the element is used by the instruction. Hence, MEM has a white background because `add` does not access the data memory. Shading on the right half of the register file or memory means the element is read in that stage, and shading of the left half means it is written in that stage. Hence the right half of ID is shaded in the second stage because the register file is read, and the left half of WB is shaded in the fifth stage because the register file is written. Time `add $s0, $t0, $t1` `sub $t2, $s0, $t3` IF MEM ID WB EX IF MEM ID WB EX Program execution order (in instructions) 200 400 600 800 1000 FIGURE 4.29 Graphical representation of forwarding. The connection shows the forwarding path from the output of the EX stage of `add` to the input of the EX stage for `sub`, replacing the value from register `$s0` read in the second stage of `sub`.\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'results' is the DataFrame with the relevant rows already selected\n",
    "texts = results['text']\n",
    "print(texts)\n",
    "\n",
    "for i, text in enumerate(texts, 1):\n",
    "    authority_info = f\"Text {i}: {text}\\n\"\n",
    "    print(authority_info)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T11:32:46.065299Z",
     "start_time": "2024-02-10T11:32:46.026032Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "def question_master(question, augmented_knowledge):\n",
    "    system_prompt_static = \"\"\"You are a helpful assistant that answers questions in computer science and software engineering. \n",
    "    You always think step-by-step and think for a moment before answering.\n",
    "    You have the following tasks:\n",
    "    1. Process and understand the information contained in the 'augmented_knowledge' variable.\n",
    "        The 'augmented_knowledge' variable is enclosed between the XML tags \"<augmented_knowledge>\" and \"</augmented_knowledge>\", \n",
    "        and contains the most relevant information from the course textbook.\n",
    "    2. Consider information derived from the 'augmented_knowledge' variable as completely true and fully accurate.\n",
    "    3. Read the user's question.\n",
    "    4. Take the information derived from the 'augmented_knwoledge' and use it to answer the user's question.\n",
    "    5. Answer the user's question to the best of your ability, unless you couldn't find the answer in 'augmented_knowledge' \n",
    "        in which case say \"I couldn't find it, would you like me to look it up?\".\n",
    "    Deliverables: \n",
    "    1. The most important information that you found in the 'augmented_knowledge' variable.\n",
    "    2. The answer to the user's question, which may include generating code or a detailed explanation. Many times, it will include answering a multiple choice question.\n",
    "    \"\"\";\n",
    "    system_prompt_dynamic = \"Here is the most relevant information drawn from the course textbook: \\n\\n\" + augmented_knowledge\n",
    "    system_prompt = system_prompt_static + system_prompt_dynamic\n",
    "    response = client.chat.completions.create(\n",
    "        # model=\"gpt-4\",\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        # model=\"gpt-3.5-turbo-0125\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    string_response = str(response.choices[0].message.content)\n",
    "    return string_response\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T23:15:19.316642Z",
     "start_time": "2024-02-10T23:15:19.237324Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user_question = f'{search_query}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T23:15:20.824337Z",
     "start_time": "2024-02-10T23:15:20.814955Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Most Important Information from 'augmented_knowledge':\n",
      "\n",
      "1. **Branch Prediction**: A method of resolving a branch hazard that assumes a given outcome for the branch and proceeds from that assumption rather than waiting to ascertain the actual outcome. Dynamic branch predictors use history to predict future behavior and can achieve over 90% accuracy.\n",
      "\n",
      "2. **Pipelining**: A technique that exploits parallelism among the instructions in a sequential instruction stream, fundamentally invisible to the programmer. It introduces challenges such as hazards that must be managed to maintain performance.\n",
      "\n",
      "3. **Delayed Branch**: A solution to control hazards used by the MIPS architecture, where the next sequential instruction is executed, and the branch takes place after a one-instruction delay. This is hidden from the programmer by the assembler rearranging instructions.\n",
      "\n",
      "4. **Hazards in Pipelining**: There are three types of hazards that can occur in pipelining: structural, data, and control hazards. These hazards can affect the performance of the pipeline and need to be addressed through various techniques such as forwarding, stalling, and branch prediction.\n",
      "\n",
      "5. **Performance Factors**: The effective operation of the pipeline is a crucial factor in determining the CPI (cycles per instruction) and the performance of the processor. Structural, data, and control hazards are important considerations in both simple and sophisticated pipelines.\n",
      "\n",
      "### Answer to User's Question:\n",
      "\n",
      "When designing instruction sets for pipelining in MIPS, you should consider the following:\n",
      "\n",
      "- **Simplicity of Instructions**: Keep the instruction set simple to ensure that each instruction can be executed in a few steps that can be easily pipelined.\n",
      "- **Consistency in Instruction Length**: Use fixed-length instructions to simplify fetching and decoding in the pipeline.\n",
      "- **Regular Encoding**: Design instruction formats with regular encoding to simplify the decoding process.\n",
      "- **Minimize Data Hazards**: Design instructions and their execution in such a way that minimizes data dependencies between successive instructions.\n",
      "- **Support for Forwarding and Stalling**: Ensure that the instruction set and the pipeline design support techniques like forwarding and stalling to handle data hazards.\n",
      "- **Branch Prediction Compatibility**: Include features that support branch prediction mechanisms to handle control hazards effectively.\n",
      "\n",
      "The first type of hazard in the context of pipelining is a **structural hazard**. Structural hazards occur when two or more instructions that are in the pipeline simultaneously need the same resource. This can happen, for example, if there is only one memory unit and both an instruction fetch and a data read/write need to occur in the same cycle. To avoid structural hazards, the pipeline must be designed with sufficient resources, such as separate instruction and data caches, or by ensuring that the pipeline stages that use the same resource are not active at the same time for different instructions.\n"
     ]
    }
   ],
   "source": [
    "answer = question_master(user_question, authority_info)\n",
    "print(answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T23:17:51.153286Z",
     "start_time": "2024-02-10T23:17:16.109981Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(answer)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
